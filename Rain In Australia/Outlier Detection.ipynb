{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db464b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_22576\\3443027451.py:12: DeprecationWarning: `import pandas_profiling` is going to be deprecated by April 1st. Please use `import ydata_profiling` instead.\n",
      "  from pandas_profiling import ProfileReport\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler\n",
    "from geopy.geocoders import Nominatim\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "import re\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.impute import SimpleImputer\n",
    "from pandas_profiling import ProfileReport\n",
    "from category_encoders.target_encoder import TargetEncoder\n",
    "import umap\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import shap\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aefad90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('data/weatherAUS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16d02ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.sort_values(['Date', 'Location'], inplace=True)\n",
    "dataset.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "550fff67",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = dataset.iloc[:109103], dataset.iloc[109103:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ce9ff91",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical = [column for column in train.columns if train.dtypes[column] == float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd936b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutlierDetection:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def z_score(self, X, threshold=3):\n",
    "        numerical = [column for column in train.columns if train.dtypes[column] == float]\n",
    "        outliers = {}\n",
    "        for col in numerical:\n",
    "            series_ = X[col].dropna()\n",
    "            mean = np.mean(series_)\n",
    "            std = np.std(series_)\n",
    "            outliers_ = series_.apply(lambda x: (x-mean)/std > threshold)\n",
    "            outliers[col] = series_[outliers_].index.values\n",
    "        return outliers\n",
    "    \n",
    "#     #possible outliers\n",
    "#     def frequency(self, X, first_n=2, return_counts=False):\n",
    "#         categorical = [column for column in train.columns if train.dtypes[column] == object]\n",
    "#         outliers = {}\n",
    "#         counts = {}\n",
    "#         for col in categorical:\n",
    "#             outliers_ = X[col].value_counts().sort_values(ascending=True)\n",
    "#             outliers[col] = outliers_.index[:first_n].values\n",
    "#             if return_counts:\n",
    "#                 counts[col] = outliers_[:first_n]\n",
    "#         if return_counts:\n",
    "#             return outliers, counts\n",
    "#         else:\n",
    "#             return outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d8f253c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = OutlierDetection()\n",
    "outliers = detector.z_score(train, threshold=3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "18437714",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_indeces = set()\n",
    "for k in outliers.keys():\n",
    "    outlier_indeces.update(outliers[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "75473877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No     2253\n",
       "Yes    2130\n",
       "Name: RainTomorrow, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[list(outlier_indeces), 'RainTomorrow'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a608ab1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self, numerical_impute_strategy, categorical_impute_strategy, roll_cols, roll_strategies, roll_period):\n",
    "        if len(roll_cols) != len(roll_strategies):\n",
    "            raise ValueError('Value Error: len(roll_cols) != len(roll_strategies).')\n",
    "        self.locations = None\n",
    "        self.numerical_impute_strategy = numerical_impute_strategy\n",
    "        self.categorical_impute_strategy = categorical_impute_strategy\n",
    "        self.roll_cols = roll_cols\n",
    "        self.roll_strategies = roll_strategies\n",
    "        self.roll_period = roll_period\n",
    "\n",
    "    def preprocess(self, _data, visualize=False):\n",
    "        data = _data.copy(deep=True)\n",
    "        \n",
    "        detector = OutlierDetection()\n",
    "        outliers = detector.z_score(data)\n",
    "        outlier_indeces = set()\n",
    "        for k in outliers.keys():\n",
    "            outlier_indeces.update(outliers[k])\n",
    "        \n",
    "        data['is_outlier'] = 0\n",
    "        data.loc[list(outlier_indeces), 'is_outlier'] = 1\n",
    "\n",
    "        for column in ['RainToday', 'RainTomorrow']:\n",
    "            data.loc[data[column] == 'Yes', column] = 1\n",
    "            data.loc[data[column] == 'No', column] = 0\n",
    "\n",
    "        data['Year'] = pd.DatetimeIndex(data['Date']).year\n",
    "        data['Month'] = pd.DatetimeIndex(data['Date']).month\n",
    "\n",
    "        if not visualize:\n",
    "            self.impute(data)\n",
    "\n",
    "        data['Location'] = data['Location'].apply(lambda loc: ' '.join(re.findall('[A-Z][^A-Z]+|[A-Z]+', loc)))\n",
    "        data.loc[data.Location == 'Portland', 'Location'] = 'Portland, Victoria'\n",
    "        data.loc[data.Location == 'Dartmoor', 'Location'] = 'Dartmoor, Victoria'\n",
    "        data.loc[data.Location == 'Perth', 'Location'] = 'Perth, Western Australia'\n",
    "        data.loc[data.Location == 'Richmond', 'Location'] = 'Richmond, New South Wales'\n",
    "        data['Location'] = data.Location + ', Australia'\n",
    "\n",
    "        if self.locations is None:\n",
    "            geolocator = Nominatim(user_agent=\"rain-in-australia-app\")\n",
    "            locations = {'Location_reduced': [], 'Location': [], 'Address': [], 'Latitude': [], 'Longitude': []}\n",
    "            for location in data.Location.unique().tolist() + ['Australia']:\n",
    "                location_enc = geolocator.geocode(location, language='en')\n",
    "                if location_enc is None:\n",
    "                    raise ValueError(f'Location not found: {location}')\n",
    "                locations['Location_reduced'] += [location.split(', ')[0]]\n",
    "                locations['Location'] += [location]\n",
    "                locations['Address'] += [location_enc.address]\n",
    "                locations['Latitude'] += [location_enc.latitude]\n",
    "                locations['Longitude'] += [location_enc.longitude]\n",
    "            self.locations = pd.DataFrame(locations)\n",
    "        data = data.merge(self.locations[['Location', 'Latitude', 'Longitude']], left_on='Location',\n",
    "                          right_on='Location')\n",
    "\n",
    "        if visualize:\n",
    "            data['Location'] = data['Location'].apply(lambda loc: loc.split(', ')[0])\n",
    "\n",
    "        data = self.RainToday_Locations(data)\n",
    "\n",
    "        if not visualize:\n",
    "            data = self.target_encoding(data)\n",
    "\n",
    "        for i, col in enumerate(self.roll_cols):\n",
    "            data[f'{col}_{self.roll_period}days'] = Preprocessor.rolling_features_for_all_locations(data[col],\n",
    "                                                                                                    data.Location,\n",
    "                                                                                                    period=self.roll_period,\n",
    "                                                                                                    shift=1,\n",
    "                                                                                                    strategy=\n",
    "                                                                                                    self.roll_strategies[\n",
    "                                                                                                        i])\n",
    "\n",
    "        acc_col = [column for column in data.columns if 'RainToday_' in column and 'days' not in column or column == 'Month_Location']\n",
    "        data['Accumulated_probabilities'] = data.loc[:, acc_col].sum(axis=1)\n",
    "        if not visualize:\n",
    "            data = self.__relevant_locations(data=data, _data=_data)\n",
    "        return data\n",
    "    \n",
    "    def __relevant_locations(self, data, _data):\n",
    "        \n",
    "        def cond(x):\n",
    "            return x[1] > 0 and ('RainToday_' in x[0] and 'days' not in x[0])\n",
    "        \n",
    "        with open('relevant_locations.pickle', 'rb') as handle:\n",
    "            relevant_locations = pickle.load(handle)\n",
    "        \n",
    "        # Top 5 locations with highest shap-values for each location:\n",
    "        _relevant_locations = {}\n",
    "        for loc in relevant_locations.keys():\n",
    "            l = list(relevant_locations[loc][np.apply_along_axis(cond, 1, relevant_locations[loc])])\n",
    "            l.sort(key=lambda x: x[1], reverse=True)\n",
    "            _relevant_locations[loc] = np.array(l)[:5, 0]\n",
    "                \n",
    "        data['Relevant_locations_probabilities'] = None\n",
    "        for loc in _relevant_locations.keys():\n",
    "            data.loc[_data.Location == loc, 'Relevant_locations_probabilities'] = data.loc[_data.Location == loc, _relevant_locations[loc]].sum(axis=1)\n",
    "        \n",
    "        raintodaycols = [column for column in data.columns if 'RainToday_' in column and 'days' not in column]\n",
    "        train_raintoday = self.preprocess(_data, visualize=True)[raintodaycols]\n",
    "        closest_locations = {}\n",
    "        for loc in relevant_locations.keys():\n",
    "            selected = train_raintoday.loc[(_data.Location == loc) & (_data.RainTomorrow == 'Yes'), raintodaycols].sum(axis=0).sort_values(ascending=False).index[:7]\n",
    "            closest_locations[loc] = selected\n",
    "        \n",
    "        data['Closest_locations_probabilities'] = None\n",
    "        for loc in closest_locations.keys():\n",
    "            data.loc[_data.Location == loc, 'Closest_locations_probabilities'] = data.loc[_data.Location == loc, closest_locations[loc]].sum(axis=1)\n",
    "        \n",
    "        data['Closest_locations_probabilities'] = data['Closest_locations_probabilities'].astype(float)\n",
    "        data['Relevant_locations_probabilities'] = data['Relevant_locations_probabilities'].astype(float)\n",
    "        \n",
    "        return data\n",
    "\n",
    "\n",
    "    def impute(self, data):\n",
    "        columns = data.columns[2:-2]\n",
    "        dtypes = data.dtypes[2:-2]\n",
    "        numerical = [column for idx, column in enumerate(columns) if dtypes[idx] == float]\n",
    "        categorical = [column for idx, column in enumerate(columns) if dtypes[idx] == object]\n",
    "        if self.numerical_impute_strategy == 'mean':\n",
    "            data[numerical] = data.groupby(['Month', 'Location'])[numerical].transform(lambda x: x.fillna(x.mean()))\n",
    "        elif self.numerical_impute_strategy == 'median':\n",
    "            data[numerical] = data.groupby(['Month', 'Location'])[numerical].transform(lambda x: x.fillna(x.median()))\n",
    "        else:\n",
    "            raise ValueError('Wrong numerical impute strategy.')\n",
    "\n",
    "        if self.categorical_impute_strategy == 'mode':\n",
    "            data[categorical] = data.groupby(['Month', 'Location'])[categorical].transform(lambda x: x.fillna(x.mode()))\n",
    "        else:\n",
    "            raise ValueError('Wrong categorical impute strategy.')\n",
    "\n",
    "    def RainToday_Locations(self, data):\n",
    "        grouped = data.groupby('Date')[['Location', 'RainToday']].apply(lambda r: r.set_index('Location').T)\n",
    "        grouped = grouped.reset_index().drop('level_1', axis=1)\n",
    "        if ',' in grouped.columns[1]:\n",
    "            grouped.columns = ['Date'] + ['RainToday_' + col[:col.index(',')] for col in grouped.columns[1:]]\n",
    "        else:\n",
    "            grouped.columns = ['Date'] + ['RainToday_' + col for col in grouped.columns[1:]]\n",
    "        return data.merge(grouped, left_on='Date', right_on='Date')\n",
    "\n",
    "    def target_encoding(self, _data):\n",
    "        data = _data.copy(deep=True)\n",
    "        data['Month_Location'] = data['Month']\n",
    "        location_cols = [column for column in data.columns if 'RainToday_' in column] + ['WindGustDir', 'WindDir9am',\n",
    "                                                                                         'WindDir3pm', 'Month_Location']\n",
    "\n",
    "        for location in data.Location.unique():\n",
    "            location_encoder = TargetEncoder(cols=location_cols, handle_missing=0)\n",
    "            location_df = data.loc[data.Location == location]\n",
    "            data.loc[data.Location == location] = location_encoder.fit_transform(location_df, location_df.RainTomorrow)\n",
    "\n",
    "        location_encoder = TargetEncoder(cols=['Location', 'Month'])\n",
    "        encoded = location_encoder.fit_transform(data, data.RainTomorrow)\n",
    "\n",
    "        for c in ['WindGustDir', 'WindDir9am', 'WindDir3pm']:\n",
    "            encoded[c] = encoded[c].astype(float)\n",
    "        return encoded\n",
    "\n",
    "    @staticmethod\n",
    "    def rolling_features_for_all_locations(series_, locations, period, shift, strategy):\n",
    "\n",
    "        def rolling_features_for_location(series_, period, shift, strategy):\n",
    "            first_n = []\n",
    "            for i in range(period):\n",
    "                if i == 0:\n",
    "                    first_n += [series_[i]]\n",
    "                else:\n",
    "                    if strategy == 'mean':\n",
    "                        first_n += [series_[:i + 1 - shift].mean()]\n",
    "                    elif strategy == 'median':\n",
    "                        first_n += [series_[:i + 1 - shift].median()]\n",
    "                    elif strategy == 'sum':\n",
    "                        first_n += [series_[:i + 1 - shift].sum()]\n",
    "                    else:\n",
    "                        raise ValueError('Wrong strategy.')\n",
    "\n",
    "            if strategy == 'mean':\n",
    "                new_series = series_.shift(shift).rolling(period).mean()\n",
    "            elif strategy == 'median':\n",
    "                new_series = series_.shift(shift).rolling(period).median()\n",
    "            elif strategy == 'sum':\n",
    "                new_series = series_.shift(shift).rolling(period).sum()\n",
    "            else:\n",
    "                raise ValueError('Wrong strategy.')\n",
    "\n",
    "            new_series[:period] = first_n\n",
    "            return new_series\n",
    "\n",
    "        all_locations = []\n",
    "        for location in locations.unique():\n",
    "            one_location, index = series_[locations == location].reset_index(drop=True, inplace=False), series_[\n",
    "                locations == location].index.to_series()\n",
    "            new_series_for_location = rolling_features_for_location(one_location, period, shift, strategy)\n",
    "            new_series_for_location.index = index\n",
    "            all_locations += [new_series_for_location]\n",
    "        return pd.concat(all_locations, axis=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_and_split():\n",
    "        dataset = pd.read_csv('data/weatherAUS.csv')\n",
    "        dataset.sort_values(['Date', 'Location'], inplace=True)\n",
    "        dataset.reset_index(drop=True, inplace=True)\n",
    "        train, test = dataset.iloc[:109103], dataset.iloc[109103:]\n",
    "        return train, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "69d4da49",
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_roll_cols = ['MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation', 'Sunshine',\n",
    "                      'WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am',\n",
    "                      'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Cloud9am',\n",
    "                      'Cloud3pm', 'Temp9am', 'Temp3pm', 'RainToday']\n",
    "standard_roll_strategies = ['mean'] * (len(standard_roll_cols) - 1) + ['sum']\n",
    "standard_roll_period = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "26e992e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mean of empty slice\n",
      "Mean of empty slice\n"
     ]
    }
   ],
   "source": [
    "preprocessor = Preprocessor('median', 'mode', standard_roll_cols, standard_roll_strategies, standard_roll_period)\n",
    "train, test = preprocessor.load_and_split()\n",
    "train_prep = preprocessor.preprocess(train, visualize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "10737440",
   "metadata": {},
   "outputs": [],
   "source": [
    "raintodaycols = [column for column in train_prep.columns if 'RainToday_' in column and 'days' not in column]\n",
    "train_prep = train_prep.drop(raintodaycols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2f26b1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prep.drop('Date', axis=1, inplace=True)\n",
    "train_X, train_y = train_prep.drop('RainTomorrow', axis=1), train_prep.RainTomorrow\n",
    "fixed_skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "414f010e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found `n_estimators` in params. Will use it instead of argument\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 15 rounds\n",
      "[15]\tcv_agg's train auc: 0.862568 + 0.000484577\tcv_agg's valid auc: 0.85999 + 0.00314946\n",
      "[30]\tcv_agg's train auc: 0.869261 + 0.000560715\tcv_agg's valid auc: 0.866452 + 0.00284573\n",
      "[45]\tcv_agg's train auc: 0.875386 + 0.000479688\tcv_agg's valid auc: 0.872191 + 0.00271171\n",
      "[60]\tcv_agg's train auc: 0.879677 + 0.000501246\tcv_agg's valid auc: 0.87617 + 0.002485\n",
      "[75]\tcv_agg's train auc: 0.883302 + 0.000449105\tcv_agg's valid auc: 0.879502 + 0.0025331\n",
      "[90]\tcv_agg's train auc: 0.886566 + 0.000390567\tcv_agg's valid auc: 0.882426 + 0.00256822\n",
      "[105]\tcv_agg's train auc: 0.889097 + 0.000396595\tcv_agg's valid auc: 0.884639 + 0.00256166\n",
      "[120]\tcv_agg's train auc: 0.89112 + 0.000439255\tcv_agg's valid auc: 0.886264 + 0.00245596\n",
      "[135]\tcv_agg's train auc: 0.892837 + 0.00046287\tcv_agg's valid auc: 0.887666 + 0.00239201\n",
      "[150]\tcv_agg's train auc: 0.894342 + 0.000452479\tcv_agg's valid auc: 0.888874 + 0.00239297\n",
      "[165]\tcv_agg's train auc: 0.895604 + 0.000466294\tcv_agg's valid auc: 0.889814 + 0.00236233\n",
      "[180]\tcv_agg's train auc: 0.896825 + 0.000493214\tcv_agg's valid auc: 0.890728 + 0.00230475\n",
      "[195]\tcv_agg's train auc: 0.897882 + 0.00045115\tcv_agg's valid auc: 0.891433 + 0.00231338\n",
      "[210]\tcv_agg's train auc: 0.898841 + 0.000429052\tcv_agg's valid auc: 0.892061 + 0.00228462\n",
      "[225]\tcv_agg's train auc: 0.899778 + 0.000448346\tcv_agg's valid auc: 0.892663 + 0.00225043\n",
      "[240]\tcv_agg's train auc: 0.900691 + 0.000454257\tcv_agg's valid auc: 0.893204 + 0.00227501\n",
      "[255]\tcv_agg's train auc: 0.901616 + 0.000443956\tcv_agg's valid auc: 0.893813 + 0.00227619\n",
      "[270]\tcv_agg's train auc: 0.902401 + 0.000465755\tcv_agg's valid auc: 0.894271 + 0.00227392\n",
      "[285]\tcv_agg's train auc: 0.903244 + 0.000504771\tcv_agg's valid auc: 0.894823 + 0.0022091\n",
      "[300]\tcv_agg's train auc: 0.903973 + 0.000493644\tcv_agg's valid auc: 0.895246 + 0.00218674\n",
      "[315]\tcv_agg's train auc: 0.904696 + 0.00049084\tcv_agg's valid auc: 0.895674 + 0.00216481\n",
      "[330]\tcv_agg's train auc: 0.90531 + 0.0004782\tcv_agg's valid auc: 0.895966 + 0.00216299\n",
      "[345]\tcv_agg's train auc: 0.905969 + 0.000479533\tcv_agg's valid auc: 0.896317 + 0.00217015\n",
      "[360]\tcv_agg's train auc: 0.906584 + 0.000486969\tcv_agg's valid auc: 0.896602 + 0.00214901\n",
      "[375]\tcv_agg's train auc: 0.907185 + 0.000506736\tcv_agg's valid auc: 0.896897 + 0.00213251\n",
      "[390]\tcv_agg's train auc: 0.907823 + 0.000498986\tcv_agg's valid auc: 0.897198 + 0.00216934\n",
      "[405]\tcv_agg's train auc: 0.908378 + 0.000495131\tcv_agg's valid auc: 0.897461 + 0.00219446\n",
      "[420]\tcv_agg's train auc: 0.908922 + 0.000526811\tcv_agg's valid auc: 0.897716 + 0.00218072\n",
      "[435]\tcv_agg's train auc: 0.90949 + 0.000533541\tcv_agg's valid auc: 0.897956 + 0.00220931\n",
      "[450]\tcv_agg's train auc: 0.910087 + 0.000529775\tcv_agg's valid auc: 0.89826 + 0.0022025\n",
      "[465]\tcv_agg's train auc: 0.910616 + 0.000537709\tcv_agg's valid auc: 0.898508 + 0.00220116\n",
      "[480]\tcv_agg's train auc: 0.911092 + 0.000514213\tcv_agg's valid auc: 0.898707 + 0.00220755\n",
      "[495]\tcv_agg's train auc: 0.911592 + 0.000499426\tcv_agg's valid auc: 0.898927 + 0.00218396\n",
      "[510]\tcv_agg's train auc: 0.912073 + 0.000501295\tcv_agg's valid auc: 0.899118 + 0.00214784\n",
      "[525]\tcv_agg's train auc: 0.912556 + 0.00050823\tcv_agg's valid auc: 0.899289 + 0.00214182\n",
      "[540]\tcv_agg's train auc: 0.913024 + 0.000505725\tcv_agg's valid auc: 0.89945 + 0.00210165\n",
      "[555]\tcv_agg's train auc: 0.91351 + 0.000512842\tcv_agg's valid auc: 0.899621 + 0.00209261\n",
      "[570]\tcv_agg's train auc: 0.914007 + 0.00053955\tcv_agg's valid auc: 0.899795 + 0.00207491\n",
      "[585]\tcv_agg's train auc: 0.914453 + 0.000552891\tcv_agg's valid auc: 0.899957 + 0.00207541\n",
      "[600]\tcv_agg's train auc: 0.914904 + 0.000551821\tcv_agg's valid auc: 0.900115 + 0.00211527\n",
      "[615]\tcv_agg's train auc: 0.915359 + 0.000581667\tcv_agg's valid auc: 0.900257 + 0.00211058\n",
      "[630]\tcv_agg's train auc: 0.915771 + 0.000561373\tcv_agg's valid auc: 0.900354 + 0.00212829\n",
      "[645]\tcv_agg's train auc: 0.916225 + 0.000564092\tcv_agg's valid auc: 0.900535 + 0.00211612\n",
      "[660]\tcv_agg's train auc: 0.916662 + 0.000579402\tcv_agg's valid auc: 0.900665 + 0.00211688\n",
      "[675]\tcv_agg's train auc: 0.917098 + 0.000605395\tcv_agg's valid auc: 0.900787 + 0.00209528\n",
      "[690]\tcv_agg's train auc: 0.917525 + 0.000601685\tcv_agg's valid auc: 0.900926 + 0.00207803\n",
      "[705]\tcv_agg's train auc: 0.917915 + 0.000605597\tcv_agg's valid auc: 0.901044 + 0.00210216\n",
      "[720]\tcv_agg's train auc: 0.918294 + 0.00061589\tcv_agg's valid auc: 0.901182 + 0.00207736\n",
      "[735]\tcv_agg's train auc: 0.9187 + 0.000608366\tcv_agg's valid auc: 0.901282 + 0.00207709\n",
      "[750]\tcv_agg's train auc: 0.919091 + 0.00059581\tcv_agg's valid auc: 0.901382 + 0.00207012\n",
      "[765]\tcv_agg's train auc: 0.919474 + 0.000592503\tcv_agg's valid auc: 0.901465 + 0.00207268\n",
      "[780]\tcv_agg's train auc: 0.919856 + 0.00059659\tcv_agg's valid auc: 0.901581 + 0.00209296\n",
      "[795]\tcv_agg's train auc: 0.920264 + 0.00058552\tcv_agg's valid auc: 0.901715 + 0.0020939\n",
      "[810]\tcv_agg's train auc: 0.920656 + 0.000590634\tcv_agg's valid auc: 0.901829 + 0.00209583\n",
      "[825]\tcv_agg's train auc: 0.921046 + 0.000582519\tcv_agg's valid auc: 0.901915 + 0.00209181\n",
      "[840]\tcv_agg's train auc: 0.921416 + 0.000563968\tcv_agg's valid auc: 0.902025 + 0.00210902\n",
      "[855]\tcv_agg's train auc: 0.921762 + 0.000577894\tcv_agg's valid auc: 0.902107 + 0.00210849\n",
      "[870]\tcv_agg's train auc: 0.922137 + 0.000556914\tcv_agg's valid auc: 0.902224 + 0.00210239\n",
      "[885]\tcv_agg's train auc: 0.922502 + 0.000566932\tcv_agg's valid auc: 0.902335 + 0.00211872\n",
      "[900]\tcv_agg's train auc: 0.922851 + 0.00057121\tcv_agg's valid auc: 0.902395 + 0.00210344\n",
      "[915]\tcv_agg's train auc: 0.923223 + 0.00055561\tcv_agg's valid auc: 0.902473 + 0.00209297\n",
      "[930]\tcv_agg's train auc: 0.923595 + 0.00053693\tcv_agg's valid auc: 0.902591 + 0.00211428\n",
      "[945]\tcv_agg's train auc: 0.923976 + 0.0005493\tcv_agg's valid auc: 0.902669 + 0.00210343\n",
      "[960]\tcv_agg's train auc: 0.924331 + 0.000541288\tcv_agg's valid auc: 0.902724 + 0.00210623\n",
      "[975]\tcv_agg's train auc: 0.924686 + 0.000534246\tcv_agg's valid auc: 0.902795 + 0.00210113\n",
      "[990]\tcv_agg's train auc: 0.925016 + 0.000552654\tcv_agg's valid auc: 0.902859 + 0.00206788\n",
      "[1005]\tcv_agg's train auc: 0.925357 + 0.000562703\tcv_agg's valid auc: 0.902928 + 0.00208132\n",
      "[1020]\tcv_agg's train auc: 0.925686 + 0.000561878\tcv_agg's valid auc: 0.90302 + 0.00206984\n",
      "[1035]\tcv_agg's train auc: 0.926011 + 0.000562904\tcv_agg's valid auc: 0.90308 + 0.00210113\n",
      "[1050]\tcv_agg's train auc: 0.926356 + 0.000534508\tcv_agg's valid auc: 0.903162 + 0.00212135\n",
      "[1065]\tcv_agg's train auc: 0.926681 + 0.000528549\tcv_agg's valid auc: 0.90321 + 0.00213503\n",
      "[1080]\tcv_agg's train auc: 0.927014 + 0.000520589\tcv_agg's valid auc: 0.903291 + 0.00210874\n",
      "[1095]\tcv_agg's train auc: 0.927338 + 0.00050297\tcv_agg's valid auc: 0.903345 + 0.00213464\n",
      "[1110]\tcv_agg's train auc: 0.927671 + 0.000494054\tcv_agg's valid auc: 0.903432 + 0.00215097\n",
      "[1125]\tcv_agg's train auc: 0.927985 + 0.000506507\tcv_agg's valid auc: 0.903472 + 0.00216355\n",
      "[1140]\tcv_agg's train auc: 0.92831 + 0.00051352\tcv_agg's valid auc: 0.903533 + 0.00213699\n",
      "[1155]\tcv_agg's train auc: 0.92862 + 0.000507646\tcv_agg's valid auc: 0.90357 + 0.00213414\n",
      "[1170]\tcv_agg's train auc: 0.92894 + 0.000485996\tcv_agg's valid auc: 0.903625 + 0.00213363\n",
      "[1185]\tcv_agg's train auc: 0.929236 + 0.000481649\tcv_agg's valid auc: 0.903687 + 0.00213664\n",
      "[1200]\tcv_agg's train auc: 0.929553 + 0.000458052\tcv_agg's valid auc: 0.903727 + 0.00214262\n",
      "[1215]\tcv_agg's train auc: 0.929832 + 0.000467832\tcv_agg's valid auc: 0.903759 + 0.00214976\n",
      "[1230]\tcv_agg's train auc: 0.930139 + 0.000439365\tcv_agg's valid auc: 0.9038 + 0.00215669\n",
      "[1245]\tcv_agg's train auc: 0.930444 + 0.000447901\tcv_agg's valid auc: 0.903835 + 0.00213451\n",
      "[1260]\tcv_agg's train auc: 0.930763 + 0.000446515\tcv_agg's valid auc: 0.903882 + 0.00214435\n",
      "[1275]\tcv_agg's train auc: 0.931041 + 0.000462192\tcv_agg's valid auc: 0.903935 + 0.0021538\n",
      "[1290]\tcv_agg's train auc: 0.931343 + 0.000460576\tcv_agg's valid auc: 0.904013 + 0.00213433\n",
      "[1305]\tcv_agg's train auc: 0.931625 + 0.000460242\tcv_agg's valid auc: 0.904051 + 0.00212293\n",
      "[1320]\tcv_agg's train auc: 0.931925 + 0.000464537\tcv_agg's valid auc: 0.904103 + 0.00214178\n",
      "[1335]\tcv_agg's train auc: 0.932217 + 0.000474779\tcv_agg's valid auc: 0.904144 + 0.00211412\n",
      "[1350]\tcv_agg's train auc: 0.932505 + 0.000480754\tcv_agg's valid auc: 0.904185 + 0.00210579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1365]\tcv_agg's train auc: 0.932779 + 0.000482478\tcv_agg's valid auc: 0.904227 + 0.00209373\n",
      "[1380]\tcv_agg's train auc: 0.93306 + 0.000464325\tcv_agg's valid auc: 0.904265 + 0.00209926\n",
      "[1395]\tcv_agg's train auc: 0.933353 + 0.000465648\tcv_agg's valid auc: 0.904313 + 0.00210579\n",
      "[1410]\tcv_agg's train auc: 0.933649 + 0.000456441\tcv_agg's valid auc: 0.904353 + 0.0021244\n",
      "[1425]\tcv_agg's train auc: 0.933923 + 0.000459408\tcv_agg's valid auc: 0.904397 + 0.00214363\n",
      "[1440]\tcv_agg's train auc: 0.934179 + 0.000445413\tcv_agg's valid auc: 0.904422 + 0.00215086\n",
      "[1455]\tcv_agg's train auc: 0.934455 + 0.000445095\tcv_agg's valid auc: 0.904427 + 0.00215028\n",
      "[1470]\tcv_agg's train auc: 0.934759 + 0.000429006\tcv_agg's valid auc: 0.904478 + 0.00214985\n",
      "[1485]\tcv_agg's train auc: 0.935028 + 0.000432734\tcv_agg's valid auc: 0.904508 + 0.00214636\n",
      "[1500]\tcv_agg's train auc: 0.935294 + 0.000415123\tcv_agg's valid auc: 0.904557 + 0.00216972\n",
      "[1515]\tcv_agg's train auc: 0.93555 + 0.000415075\tcv_agg's valid auc: 0.904587 + 0.00219129\n",
      "[1530]\tcv_agg's train auc: 0.935818 + 0.000429213\tcv_agg's valid auc: 0.904639 + 0.00217421\n",
      "[1545]\tcv_agg's train auc: 0.936088 + 0.00042883\tcv_agg's valid auc: 0.904685 + 0.00216794\n",
      "[1560]\tcv_agg's train auc: 0.936359 + 0.000440449\tcv_agg's valid auc: 0.904742 + 0.00217231\n",
      "[1575]\tcv_agg's train auc: 0.936625 + 0.000443022\tcv_agg's valid auc: 0.90477 + 0.0021812\n",
      "[1590]\tcv_agg's train auc: 0.93689 + 0.000446287\tcv_agg's valid auc: 0.904815 + 0.00218426\n",
      "[1605]\tcv_agg's train auc: 0.937138 + 0.000436658\tcv_agg's valid auc: 0.904861 + 0.0021793\n",
      "[1620]\tcv_agg's train auc: 0.937403 + 0.00043115\tcv_agg's valid auc: 0.904891 + 0.00218069\n",
      "[1635]\tcv_agg's train auc: 0.93765 + 0.000427562\tcv_agg's valid auc: 0.904915 + 0.00217795\n",
      "[1650]\tcv_agg's train auc: 0.937892 + 0.000394247\tcv_agg's valid auc: 0.904952 + 0.00217258\n",
      "[1665]\tcv_agg's train auc: 0.93815 + 0.000379391\tcv_agg's valid auc: 0.904978 + 0.00218412\n",
      "[1680]\tcv_agg's train auc: 0.938422 + 0.000376733\tcv_agg's valid auc: 0.904982 + 0.00217076\n",
      "[1695]\tcv_agg's train auc: 0.938672 + 0.00038949\tcv_agg's valid auc: 0.905014 + 0.00219523\n",
      "[1710]\tcv_agg's train auc: 0.938932 + 0.000386296\tcv_agg's valid auc: 0.905032 + 0.00217706\n",
      "[1725]\tcv_agg's train auc: 0.93917 + 0.000389343\tcv_agg's valid auc: 0.905072 + 0.00217577\n",
      "[1740]\tcv_agg's train auc: 0.939427 + 0.000416956\tcv_agg's valid auc: 0.905094 + 0.00218884\n",
      "[1755]\tcv_agg's train auc: 0.939685 + 0.000412649\tcv_agg's valid auc: 0.90512 + 0.00219576\n",
      "[1770]\tcv_agg's train auc: 0.939942 + 0.000425291\tcv_agg's valid auc: 0.905158 + 0.00219437\n",
      "[1785]\tcv_agg's train auc: 0.940177 + 0.000408985\tcv_agg's valid auc: 0.905166 + 0.00219259\n",
      "[1800]\tcv_agg's train auc: 0.940424 + 0.000403069\tcv_agg's valid auc: 0.905177 + 0.00220689\n",
      "[1815]\tcv_agg's train auc: 0.940662 + 0.00041165\tcv_agg's valid auc: 0.905173 + 0.00218245\n",
      "[1830]\tcv_agg's train auc: 0.940888 + 0.000399582\tcv_agg's valid auc: 0.9052 + 0.00222773\n",
      "[1845]\tcv_agg's train auc: 0.941139 + 0.000403907\tcv_agg's valid auc: 0.905214 + 0.00223134\n",
      "[1860]\tcv_agg's train auc: 0.941376 + 0.000396366\tcv_agg's valid auc: 0.905264 + 0.00223667\n",
      "[1875]\tcv_agg's train auc: 0.941621 + 0.000399622\tcv_agg's valid auc: 0.90529 + 0.00221764\n",
      "[1890]\tcv_agg's train auc: 0.941856 + 0.000390081\tcv_agg's valid auc: 0.905309 + 0.00222374\n",
      "[1905]\tcv_agg's train auc: 0.942111 + 0.000397247\tcv_agg's valid auc: 0.905331 + 0.00224665\n",
      "[1920]\tcv_agg's train auc: 0.942327 + 0.000409952\tcv_agg's valid auc: 0.905346 + 0.00225347\n",
      "[1935]\tcv_agg's train auc: 0.942541 + 0.000400612\tcv_agg's valid auc: 0.905383 + 0.00223814\n",
      "[1950]\tcv_agg's train auc: 0.94277 + 0.000398702\tcv_agg's valid auc: 0.905428 + 0.00224875\n",
      "[1965]\tcv_agg's train auc: 0.942999 + 0.000393564\tcv_agg's valid auc: 0.905447 + 0.00224458\n",
      "[1980]\tcv_agg's train auc: 0.943236 + 0.000391998\tcv_agg's valid auc: 0.90548 + 0.00226833\n",
      "[1995]\tcv_agg's train auc: 0.943453 + 0.000382817\tcv_agg's valid auc: 0.905492 + 0.00226872\n",
      "[2010]\tcv_agg's train auc: 0.943697 + 0.000378786\tcv_agg's valid auc: 0.90552 + 0.0022792\n",
      "[2025]\tcv_agg's train auc: 0.943946 + 0.0003872\tcv_agg's valid auc: 0.905549 + 0.00228234\n",
      "[2040]\tcv_agg's train auc: 0.944186 + 0.000400997\tcv_agg's valid auc: 0.905565 + 0.00228774\n",
      "[2055]\tcv_agg's train auc: 0.944413 + 0.000387816\tcv_agg's valid auc: 0.905582 + 0.00227852\n",
      "[2070]\tcv_agg's train auc: 0.944628 + 0.000379488\tcv_agg's valid auc: 0.905578 + 0.00226027\n",
      "[2085]\tcv_agg's train auc: 0.944859 + 0.000375698\tcv_agg's valid auc: 0.905596 + 0.00226321\n",
      "[2100]\tcv_agg's train auc: 0.945095 + 0.000367961\tcv_agg's valid auc: 0.90562 + 0.00226719\n",
      "[2115]\tcv_agg's train auc: 0.94532 + 0.000377181\tcv_agg's valid auc: 0.905665 + 0.00227488\n",
      "[2130]\tcv_agg's train auc: 0.945544 + 0.000353582\tcv_agg's valid auc: 0.905692 + 0.00227964\n",
      "[2145]\tcv_agg's train auc: 0.945758 + 0.000351121\tcv_agg's valid auc: 0.905722 + 0.00229091\n",
      "[2160]\tcv_agg's train auc: 0.94598 + 0.000365314\tcv_agg's valid auc: 0.90574 + 0.00228632\n",
      "[2175]\tcv_agg's train auc: 0.946192 + 0.000363668\tcv_agg's valid auc: 0.905755 + 0.00228743\n",
      "[2190]\tcv_agg's train auc: 0.946419 + 0.000362553\tcv_agg's valid auc: 0.905777 + 0.00226702\n",
      "[2205]\tcv_agg's train auc: 0.946644 + 0.000370413\tcv_agg's valid auc: 0.9058 + 0.00226821\n",
      "[2220]\tcv_agg's train auc: 0.946846 + 0.000368569\tcv_agg's valid auc: 0.905801 + 0.0022792\n",
      "Early stopping, best iteration is:\n",
      "[2210]\tcv_agg's train auc: 0.946719 + 0.000360931\tcv_agg's valid auc: 0.905814 + 0.00227578\n"
     ]
    }
   ],
   "source": [
    "lgb_params = {\n",
    "\n",
    "    \"objective\": \"binary\",\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"num_threads\": 10,\n",
    "    \"metric\": \"AUC\",\n",
    "    \"seed\": 42,\n",
    "   \"verbose\":-1,\n",
    "    \"class_weight\": 'balanced',\n",
    "    \n",
    "     #regularization\n",
    "    \"colsample_bytree\": 0.6,\n",
    "    \"subsample\": 0.8,\n",
    "\n",
    "    \"subsample_freq\": 1,\n",
    "    \"min_data_in_leaf\": 300,\n",
    "\n",
    "    \"num_leaves\":10,\n",
    "    \n",
    "    \"n_estimators\":10_000\n",
    "    \n",
    "    #categorical features\n",
    "#     'cat_smooth': 5,\n",
    "#     'min_data_per_group': 2\n",
    "#     did not improve the results\n",
    "    \n",
    "}\n",
    "lgb_train = lgb.Dataset(train_X, label=train_y.fillna(0), free_raw_data=False)\n",
    "result = lgb.cv(lgb_params, lgb_train, 10_000, folds=fixed_skf, callbacks=[lgb.early_stopping(15), lgb.log_evaluation(15)], eval_train_metric=True, return_cvbooster=True)\n",
    "lgb_params['n_estimators'] = result[\"cvbooster\"].best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03e9c14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
